{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 챗봇 만들기\n",
    "\n",
    "### https://jfun.tistory.com/199?category=851249\n",
    "### https://www.slideshare.net/KimSungdong1/20170227-72644192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = '판교에 지금 주문해줘'\n",
    "output_data = ''\n",
    "\n",
    "request = {\n",
    "    \"intent_id\":\"\",\n",
    "    \"input_data\":input_data,\n",
    "    \"request_type\":\"text\",\n",
    "    \"story_slot_entity\":{},\n",
    "    \"output_data\":output_data\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기본 데이터 셋(DB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_list = {\n",
    "    \"주문\":[\"주문\", \"배달\"],\n",
    "    \"예약\":[\"예약\", \"잡아줘\"],\n",
    "    \"정보\":[\"정보\",\"알려\"]\n",
    "}\n",
    "\n",
    "story_slot_entity = {\"주문\":{\"메뉴\":None, \"장소\":None, \"날짜\":None},\n",
    "\"예약\":{\"장소\": None, \"날짜\":None},\n",
    "\"정보\":{\"대상\":None}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 형태소 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[('판교', 'NNG'), ('에', 'JKB'), ('지금', 'MAG'), ('주문', 'NNG'), ('해', 'XSV+EC'), ('줘', 'VX+EC')]\n"
    }
   ],
   "source": [
    "from eunjeon import Mecab\n",
    "mecab = Mecab()\n",
    "preprocessed = mecab.pos(request.get('input_data'))\n",
    "\n",
    "print(preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intent 도출(Ruled Based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Char CNN을 사용해서 연결하면 좀 더 쉽게 만들 수 있다\n",
    "intent_id = \"주문\"\n",
    "slot_value = story_slot_entity.get(\"주문\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER 도출(Ruled based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM 기법을 사용해서 연결하면 좀 더 쉬울 수 있다\n",
    "menu_list = [\"피자\", \"햄버거\", \"치킨\"]\n",
    "loc_list = [\"판교\", \"야탑\", \"서현\"]\n",
    "date_list = [\"지금\", '내일', '모레']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary 기반 slot 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'메뉴': None, '장소': '판교', '날짜': '지금'}\n"
    }
   ],
   "source": [
    "for pos_tag in preprocessed:\n",
    "    if pos_tag[1] in ['NNG', 'NNP', 'SL', 'MAG']:\n",
    "        if pos_tag[0] in menu_list:\n",
    "            slot_value['메뉴'] = pos_tag[0]\n",
    "        elif pos_tag[0] in loc_list:\n",
    "            slot_value['장소'] = pos_tag[0]\n",
    "        elif pos_tag[0] in date_list:\n",
    "            slot_value['날짜'] = pos_tag[0]\n",
    "\n",
    "\n",
    "print(story_slot_entity.get('주문'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 빈 slot 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "메뉴선택해주세요\n"
    }
   ],
   "source": [
    "if None in slot_value.values():\n",
    "    key_values=\"\"\n",
    "    for key in slot_value.keys():\n",
    "        if slot_value[key] is None:\n",
    "            key_values = key_values + key + \"\"\n",
    "    output_data = key_values + \"선택해주세요\"\n",
    "else:\n",
    "    output_data=\"주문이 완료 되었습니다.\"\n",
    "\n",
    "print(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "메뉴선택해주세요\n"
    }
   ],
   "source": [
    "response = {\n",
    "    \"intent_id\":\"\",\n",
    "    \"input_data\":input_data,\n",
    "    \"request_type\":\"text\",\n",
    "    \"story_slot_entity\":{},\n",
    "    \"output_data\":\"\"\n",
    "}\n",
    "\n",
    "response[\"output_data\"]=output_data\n",
    "\n",
    "print(response[\"output_data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['안녕', '만나', '서', '반가워', '넌', '누구', '니', '나', '는', 'AI', '봇', '이', '란다', '.', '피자', '주문', '할께', '음료', '도', '주문', '해', '줘', '음료', '는', '뭘로', '콜라', '로', '해', '줘']\n"
    }
   ],
   "source": [
    "ona_data = [\n",
    "    ['안녕', '만나서 반가워'],\n",
    "    ['넌 누구니','나는 AI봇이란다.'],\n",
    "    ['피자 주문할께', '음료도 주문해줘'],\n",
    "    ['음료는 뭘로', '콜라로 해줘']\n",
    "]\n",
    "\n",
    "train_gata = list(map(lambda x: mecab.morphs(''.join(x)), ona_data))\n",
    "\n",
    "import itertools\n",
    "\n",
    "train_data = list(itertools.chain.from_iterable(train_gata))\n",
    "\n",
    "print(list(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0.]\n[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0.]\n[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0.]\n[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0.]\n[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0.]\n[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0.]\n[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0.]\n[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0.]\n[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0.]\n[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0.]\n[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0.]\n[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0.]\n[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0.]\n[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0.]\n[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0.]\n[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0.]\n[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0.]\n[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0.]\n[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0.]\n[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0.]\n[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0.]\n[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n 0. 0. 0. 0. 0. 0.]\n[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n 0. 0. 0. 0. 0. 0.]\n[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0.]\n[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0.]\n[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 1. 0. 0. 0. 0. 0.]\n[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 1. 0. 0. 0. 0.]\n[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 1. 0. 0. 0.]\n[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n 0. 0. 0. 0. 0. 0.]\n[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n 0. 0. 0. 0. 0. 0.]\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "bucket = np.zeros(len(train_data), dtype=np.float)\n",
    "print(bucket)\n",
    "\n",
    "for word in train_data:\n",
    "    bucket_temp = bucket.copy()\n",
    "    bucket_temp = np.insert(bucket_temp, train_data.index(word), 1)\n",
    "    print(bucket_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[['안녕', '만나', '서', '반가워', '넌', '누구', '니', '나', '는', 'AI', '봇', '이', '란다', '.', '피자', '주문', '할께', '음료', '도', '주문', '해', '줘', '음료', '는', '뭘로', '콜라', '로', '해', '줘']]\nmodel check:Word2Vec(vocab=24, size=50, alpha=0.025)\n"
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "train_data = [train_data]\n",
    "print(train_data)\n",
    "\n",
    "model = word2vec.Word2Vec(size=50, window=2, min_count=1)\n",
    "model.build_vocab(train_data)\n",
    "model.train(train_data, epochs=model.iter,total_examples=model.corpus_count)\n",
    "print(\"model check:{0}\".format(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "model load check : Word2Vec(vocab=24, size=50, alpha=0.025)\n"
    }
   ],
   "source": [
    "import os\n",
    "model.save(r\"E:\\Programming\\python\\NLP\\howls_nlp\\Chatbot\\자료\\models\\w2v.bin\")\n",
    "model = word2vec.Word2Vec.load(r\"E:\\Programming\\python\\NLP\\howls_nlp\\Chatbot\\자료\\models\\w2v.bin\")\n",
    "print(\"model load check : {0}\".format(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Word2Vec(vocab=24, size=50, alpha=0.025)\n['는', '주문', '음료', '해', '줘', '안녕', '만나', '서', '반가워', '넌', '누구', '니', '나', 'AI', '봇', '이', '란다', '.', '피자', '할께', '도', '뭘로', '콜라', '로']\n"
    }
   ],
   "source": [
    "X = model[]\n",
    "print(model.wv.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[-0.00976191  0.00973107  0.00252566  0.00820518 -0.00646039  0.00390535\n -0.00628025 -0.00704649 -0.00706708 -0.00498548  0.00822745  0.00053697\n  0.00817156 -0.00821855 -0.00402307 -0.00452205 -0.00876464 -0.00399296\n -0.00167992 -0.00735352  0.00124907 -0.00224733  0.00192989  0.00914724\n -0.00956092  0.00481127  0.00788746 -0.00258857 -0.00404193 -0.00562656\n -0.00486423 -0.0033742  -0.00810599  0.0035026   0.00563072  0.00184724\n  0.00378866  0.00454431  0.00513543 -0.00544446  0.00536345  0.00793488\n  0.00616158  0.00824817 -0.00291877  0.00648411  0.00172757 -0.00349816\n -0.00697151  0.00120333]\n"
    }
   ],
   "source": [
    "print(model[\"안녕\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[ 0.00988131  0.00828089  0.00588054 -0.00300516  0.00822219 -0.00365124\n -0.00801502  0.00072754 -0.00099351  0.00973184  0.0097055  -0.00023882\n  0.0067852  -0.00537235  0.00191611  0.00774843 -0.00862572  0.00496778\n -0.00430471  0.00445049 -0.00469619  0.00640069 -0.00866665 -0.00337163\n  0.00824308  0.00715727  0.00148865 -0.00540752  0.00841281  0.00533213\n  0.00207207 -0.00114708  0.00017083  0.00141542 -0.00453451  0.00323375\n -0.00685723 -0.00848806 -0.00481923  0.00331312 -0.00862415  0.00179881\n -0.00304985  0.00922066 -0.00756584  0.00641558  0.00413202 -0.00847561\n  0.00806909 -0.00329407]\n"
    }
   ],
   "source": [
    "print(model[\"AI\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[('주문', 0.29700160026550293), ('니', 0.26338884234428406), ('만나', 0.2469508945941925), ('콜라', 0.18749526143074036), ('봇', 0.1744612157344818), ('할께', 0.1415926218032837), ('나', 0.13562972843647003), ('도', 0.0877845361828804), ('이', 0.05164101719856262), ('AI', 0.02810666523873806)]\n"
    }
   ],
   "source": [
    "res1 = model.most_similar(positive=\"누구\", negative=\"\", topn=10)\n",
    "print(res1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-dcacff4e7136>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'font'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfont_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mtsne\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mX_tsne\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtsne\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_tsne\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "font_name = matplotlib.font_manager.FontProperties(\n",
    "    fname=r\"C:\\Program Files\\Android\\Android Studio\\plugins\\android\\lib\\layoutlib\\data\\fonts\\NanumGothic.ttf\"\n",
    ").get_name()\n",
    "vocab = model.wv.index2word\n",
    "matplotlib.rc('font', family=font_name)\n",
    "tsne = TSNE(n_components=2)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "df = pd.concat([pd.DataFrame(X_tsne), pd.Series(vocab)], axis=1)\n",
    "\n",
    "df.columns = ['x', 'y', 'word']\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "print(df)\n",
    "ax.scatter(df['x'], df['y'])\n",
    "ax.set_xlim(df['x'].max(), df['x'].min())\n",
    "ax.set_ylim(df['y'].max(), df['y'].min())\n",
    "for i, txt in enumerate(df['word']):\n",
    "    ax.annotate(txt, (df['x'].iloc[i], df['y'].iloc[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}